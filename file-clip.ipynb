{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":10906876,"sourceType":"datasetVersion","datasetId":6779370},{"sourceId":10909750,"sourceType":"datasetVersion","datasetId":6781576}],"dockerImageVersionId":30918,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<center><a href=\"https://www.nvidia.com/dli\"> <img src=\"images/DLI_Header.png\" alt=\"Header\" style=\"width: 400px;\"/> </a></center>","metadata":{"id":"K2-Px6LAIoz7"}},{"cell_type":"code","source":"import torch\n\n# Kiểm tra số lượng GPU khả dụng\nprint(\"Số lượng GPU khả dụng:\", torch.cuda.device_count())\n\n# Kiểm tra tên GPU\nfor i in range(torch.cuda.device_count()):\n    print(f\"GPU {i}: {torch.cuda.get_device_name(i)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-09T07:42:55.258800Z","iopub.execute_input":"2025-03-09T07:42:55.258980Z","iopub.status.idle":"2025-03-09T07:42:59.080909Z","shell.execute_reply.started":"2025-03-09T07:42:55.258962Z","shell.execute_reply":"2025-03-09T07:42:59.080139Z"}},"outputs":[{"name":"stdout","text":"Số lượng GPU khả dụng: 2\nGPU 0: Tesla T4\nGPU 1: Tesla T4\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"!pip install openai-clip\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-09T07:42:59.082413Z","iopub.execute_input":"2025-03-09T07:42:59.082962Z","iopub.status.idle":"2025-03-09T07:43:05.779140Z","shell.execute_reply.started":"2025-03-09T07:42:59.082925Z","shell.execute_reply":"2025-03-09T07:43:05.778145Z"}},"outputs":[{"name":"stdout","text":"Collecting openai-clip\n  Downloading openai-clip-1.0.1.tar.gz (1.4 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m23.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\nCollecting ftfy (from openai-clip)\n  Downloading ftfy-6.3.1-py3-none-any.whl.metadata (7.3 kB)\nRequirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from openai-clip) (2024.11.6)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai-clip) (4.67.1)\nRequirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from ftfy->openai-clip) (0.2.13)\nDownloading ftfy-6.3.1-py3-none-any.whl (44 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.8/44.8 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hBuilding wheels for collected packages: openai-clip\n  Building wheel for openai-clip (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for openai-clip: filename=openai_clip-1.0.1-py3-none-any.whl size=1368605 sha256=a15ed0cf9a693695e7e0e191d82c8e38ad6cc9a7917b6779e53a56422b7fd3ba\n  Stored in directory: /root/.cache/pip/wheels/08/77/8e/8d2f862df6bf7fb4e2007062d2cbaeae49862ec7b56d041229\nSuccessfully built openai-clip\nInstalling collected packages: ftfy, openai-clip\nSuccessfully installed ftfy-6.3.1 openai-clip-1.0.1\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"import sys\nsys.path.append(\"/kaggle/input/test-n\")\n\nimport other_utils\nimport ddpm_utils\nimport UNet_utils","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-09T07:43:05.780522Z","iopub.execute_input":"2025-03-09T07:43:05.780849Z","iopub.status.idle":"2025-03-09T07:43:08.360392Z","shell.execute_reply.started":"2025-03-09T07:43:05.780825Z","shell.execute_reply":"2025-03-09T07:43:08.359509Z"}},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":"# 5. CLIP","metadata":{}},{"cell_type":"markdown","source":"Contrastive Language-Image Pre-Training or [CLIP](https://github.com/openai/CLIP/tree/main) is a text and image encoding tool used with many popular Generative AI models such as [DALL-E](https://openai.com/dall-e-2) and [Stable Diffusion](https://github.com/Stability-AI/stablediffusion).\n\nCLIP in itself is not a Generative AI model, but is instead used to align text encodings with image encodings. If there is such a thing as the perfect text description of an image, the goal of CLIP is to create the same vector embedding for both the image and the text. Let's see what this means in practice.\n\nThe goals of this notebook are to:\n* Learn how to use CLIP Encodings\n  * Get an image encoding\n  * Get a text encoding\n  * Calculate the cosine similarity between them\n* Use CLIP to create a text-to-image neural network\n\n## 5.1 Encodings\n\nFirst, let's load the libraries needed for this exercise.","metadata":{}},{"cell_type":"code","source":"import csv\nimport glob\nimport numpy as np\nimport torch\nimport torch.nn.functional as F\nfrom torch.optim import Adam\nimport torchvision.transforms as transforms\nfrom torch.utils.data import Dataset, DataLoader\n\n# Visualization tools\nimport matplotlib.pyplot as plt\nfrom PIL import Image\nfrom torchvision.utils import save_image, make_grid\nfrom textwrap import wrap","metadata":{"id":"MWn2WgPaIoz8","trusted":true,"execution":{"iopub.status.busy":"2025-03-09T07:43:08.361237Z","iopub.execute_input":"2025-03-09T07:43:08.361650Z","iopub.status.idle":"2025-03-09T07:43:08.365954Z","shell.execute_reply.started":"2025-03-09T07:43:08.361621Z","shell.execute_reply":"2025-03-09T07:43:08.365154Z"}},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":"There are a few different variations of CLIP based on popular image recognition neural networks:","metadata":{}},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ndevice","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-09T07:43:08.366708Z","iopub.execute_input":"2025-03-09T07:43:08.366937Z","iopub.status.idle":"2025-03-09T07:43:08.389807Z","shell.execute_reply.started":"2025-03-09T07:43:08.366917Z","shell.execute_reply":"2025-03-09T07:43:08.389094Z"}},"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"device(type='cuda')"},"metadata":{}}],"execution_count":5},{"cell_type":"code","source":"import clip\nprint(clip.available_models())  # Kiểm tra danh sách mô hình CLIP có sẵn\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-09T07:43:08.391860Z","iopub.execute_input":"2025-03-09T07:43:08.392095Z","iopub.status.idle":"2025-03-09T07:43:08.766156Z","shell.execute_reply.started":"2025-03-09T07:43:08.392075Z","shell.execute_reply":"2025-03-09T07:43:08.765464Z"}},"outputs":[{"name":"stdout","text":"['RN50', 'RN101', 'RN50x4', 'RN50x16', 'RN50x64', 'ViT-B/32', 'ViT-B/16', 'ViT-L/14', 'ViT-L/14@336px']\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"clip_model, clip_preprocess = clip.load(\"ViT-B/32\")\nclip_model.eval()\nCLIP_FEATURES = 512","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-09T07:43:08.767460Z","iopub.execute_input":"2025-03-09T07:43:08.767687Z","iopub.status.idle":"2025-03-09T07:43:15.780109Z","shell.execute_reply.started":"2025-03-09T07:43:08.767668Z","shell.execute_reply":"2025-03-09T07:43:15.779162Z"}},"outputs":[{"name":"stderr","text":"100%|████████████████████████████████████████| 338M/338M [00:02<00:00, 171MiB/s]\n","output_type":"stream"}],"execution_count":7},{"cell_type":"markdown","source":"For this notebook, we will be using `ViT-B/32`, which is based on the [Vision Transformer](https://huggingface.co/docs/transformers/main/model_doc/vit) architecture. It has `512` features, which we will later feed into our diffusion model.","metadata":{}},{"cell_type":"markdown","source":"### 5.1.1 Image Encodings","metadata":{}},{"cell_type":"markdown","source":"When we load CLIP, it will also come with a set of image transformations we can use to feed images into the CLIP model:","metadata":{}},{"cell_type":"code","source":"clip_preprocess","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-09T07:43:15.781065Z","iopub.execute_input":"2025-03-09T07:43:15.781330Z","iopub.status.idle":"2025-03-09T07:43:15.785944Z","shell.execute_reply.started":"2025-03-09T07:43:15.781296Z","shell.execute_reply":"2025-03-09T07:43:15.785152Z"}},"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"Compose(\n    Resize(size=224, interpolation=bicubic, max_size=None, antialias=True)\n    CenterCrop(size=(224, 224))\n    <function _convert_image_to_rgb at 0x78699948d900>\n    ToTensor()\n    Normalize(mean=(0.48145466, 0.4578275, 0.40821073), std=(0.26862954, 0.26130258, 0.27577711))\n)"},"metadata":{}}],"execution_count":8},{"cell_type":"markdown","source":"We can test this on one of our flower photos. Let's start with a picturesque daisy.","metadata":{}},{"cell_type":"code","source":"DATA_DIR = \"/kaggle/input/hoahoa/flower_photos/\"\nimg_path = DATA_DIR + \"daisy/2877860110_a842f8b14a_m.jpg\"\nimg = Image.open(img_path)\nimg.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-09T07:43:15.786878Z","iopub.execute_input":"2025-03-09T07:43:15.787156Z","iopub.status.idle":"2025-03-09T07:43:15.882588Z","shell.execute_reply.started":"2025-03-09T07:43:15.787128Z","shell.execute_reply":"2025-03-09T07:43:15.881774Z"}},"outputs":[],"execution_count":9},{"cell_type":"markdown","source":"We can find the CLIP embedding by first transforming our image with `clip_preprocess` and converting the result to a tensor. Since the `clip_model` expects a batch of images, we can use [np.stack](https://numpy.org/doc/stable/reference/generated/numpy.stack.html) to turn the processed image into a single element batch.","metadata":{}},{"cell_type":"code","source":"clip_imgs = torch.tensor(np.stack([clip_preprocess(img)])).to(device)\nclip_imgs.size()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-09T07:43:15.883451Z","iopub.execute_input":"2025-03-09T07:43:15.883728Z","iopub.status.idle":"2025-03-09T07:43:15.907240Z","shell.execute_reply.started":"2025-03-09T07:43:15.883701Z","shell.execute_reply":"2025-03-09T07:43:15.906526Z"}},"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"torch.Size([1, 3, 224, 224])"},"metadata":{}}],"execution_count":10},{"cell_type":"markdown","source":"Then, we can pass the batch to `clip_model.encode_image` to find the embedding for the image. Uncomment `clip_img_encoding` if you would like to see what an encoding looks like. When we print the size, it lists `512` features for our `1` image.","metadata":{}},{"cell_type":"code","source":"clip_img_encoding = clip_model.encode_image(clip_imgs)\nprint(clip_img_encoding.size())\n#clip_img_encoding","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-09T07:43:15.908098Z","iopub.execute_input":"2025-03-09T07:43:15.908411Z","iopub.status.idle":"2025-03-09T07:43:16.801984Z","shell.execute_reply.started":"2025-03-09T07:43:15.908358Z","shell.execute_reply":"2025-03-09T07:43:16.801225Z"}},"outputs":[{"name":"stdout","text":"torch.Size([1, 512])\n","output_type":"stream"}],"execution_count":11},{"cell_type":"markdown","source":"### 5.1.2 Text Encodings","metadata":{}},{"cell_type":"markdown","source":"Now that we have an image encoding, let's see if we can get a matching text encoding. Below is a list of different flower descriptions. Like with the images, the text needs to be preprocessed before it can be encoded by CLIP. To do this, CLIP comes with a `tokenize` function in order to convert each word into an integer.","metadata":{}},{"cell_type":"code","source":"text_list = [\n    \"A white daisy with a yellow center\",\n    \"An orange sunflower with a big brown center\",\n    \"A red rose bud\"\n]\ntext_tokens = clip.tokenize(text_list).to(device)\ntext_tokens","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-09T07:43:16.803013Z","iopub.execute_input":"2025-03-09T07:43:16.803394Z","iopub.status.idle":"2025-03-09T07:43:16.819448Z","shell.execute_reply.started":"2025-03-09T07:43:16.803351Z","shell.execute_reply":"2025-03-09T07:43:16.818654Z"}},"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"tensor([[49406,   320,  1579, 12865,   593,   320,  4481,  2119, 49407,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0],\n        [49406,   550,  4287, 21559,   593,   320,  1205,  2866,  2119, 49407,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0],\n        [49406,   320,   736,  3568, 10737, 49407,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0]], device='cuda:0',\n       dtype=torch.int32)"},"metadata":{}}],"execution_count":12},{"cell_type":"markdown","source":"Then, we can pass the tokens to `encode_text` to get our text encodings. Uncomment `clip_text_encodings` if you would like to see what an encoding looks like. Similar to our image encoding, there are `512` features for each of our `3` images.","metadata":{}},{"cell_type":"code","source":"clip_text_encodings = clip_model.encode_text(text_tokens).float()\nprint(clip_text_encodings.size())\n#clip_text_encodings","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-09T07:43:16.820135Z","iopub.execute_input":"2025-03-09T07:43:16.820406Z","iopub.status.idle":"2025-03-09T07:43:16.974554Z","shell.execute_reply.started":"2025-03-09T07:43:16.820386Z","shell.execute_reply":"2025-03-09T07:43:16.973584Z"}},"outputs":[{"name":"stdout","text":"torch.Size([3, 512])\n","output_type":"stream"}],"execution_count":13},{"cell_type":"markdown","source":"### 5.1.3 Similarity","metadata":{}},{"cell_type":"markdown","source":"In order to see which one of our text descriptions best describes the daisy, we can calculate the [cosine similarity](https://medium.com/@milana.shxanukova15/cosine-distance-and-cosine-similarity-a5da0e4d9ded) between the text encodings and the image encodings. When the cosine similarity is `1`, it's a perfect match. When the cosine similarity is `-1`, the two encodings are opposites.\n\nThe cosine similarity is equivalent to a [dot product](https://mathworld.wolfram.com/DotProduct.html) with each vector normalized by their magnitude. In other words, the magnitude of each vector becomes `1`.\n\nWe can use the following formula to calculate the dot product:\n\n$X \\cdot Y = \\sum_{i=1}^{n} x_i y_i = x_1y_1 + x_2 y_2 + \\cdots  + x_n y_n$","metadata":{}},{"cell_type":"code","source":"clip_img_encoding /= clip_img_encoding.norm(dim=-1, keepdim=True)\nclip_text_encodings /= clip_text_encodings.norm(dim=-1, keepdim=True)\nsimilarity = (clip_text_encodings * clip_img_encoding).sum(-1)\nsimilarity","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-09T07:43:16.975555Z","iopub.execute_input":"2025-03-09T07:43:16.975887Z","iopub.status.idle":"2025-03-09T07:43:17.223606Z","shell.execute_reply.started":"2025-03-09T07:43:16.975857Z","shell.execute_reply":"2025-03-09T07:43:17.222889Z"}},"outputs":[{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"tensor([0.3543, 0.2473, 0.1768], device='cuda:0', grad_fn=<SumBackward1>)"},"metadata":{}}],"execution_count":14},{"cell_type":"markdown","source":"What do you think? Does the most descriptive text get the highest score?","metadata":{}},{"cell_type":"code","source":"for idx, text in enumerate(text_list):\n    print(text, \" - \", similarity[idx])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-09T07:43:17.224391Z","iopub.execute_input":"2025-03-09T07:43:17.224607Z","iopub.status.idle":"2025-03-09T07:43:17.234681Z","shell.execute_reply.started":"2025-03-09T07:43:17.224590Z","shell.execute_reply":"2025-03-09T07:43:17.234034Z"}},"outputs":[{"name":"stdout","text":"A white daisy with a yellow center  -  tensor(0.3543, device='cuda:0', grad_fn=<SelectBackward0>)\nAn orange sunflower with a big brown center  -  tensor(0.2473, device='cuda:0', grad_fn=<SelectBackward0>)\nA red rose bud  -  tensor(0.1768, device='cuda:0', grad_fn=<SelectBackward0>)\n","output_type":"stream"}],"execution_count":15},{"cell_type":"markdown","source":"Let's practice a little more. Below, we've added a sunflower and a rose image.","metadata":{}},{"cell_type":"code","source":"img_paths = [\n    DATA_DIR + \"daisy/2877860110_a842f8b14a_m.jpg\",\n    DATA_DIR + \"sunflowers/2721638730_34a9b7a78b.jpg\",\n    DATA_DIR + \"roses/8032328803_30afac8b07_m.jpg\"\n]\n\nimgs = [Image.open(path) for path in img_paths]\nfor img in imgs:\n    img.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-09T07:43:17.235572Z","iopub.execute_input":"2025-03-09T07:43:17.235809Z","iopub.status.idle":"2025-03-09T07:43:17.374910Z","shell.execute_reply.started":"2025-03-09T07:43:17.235791Z","shell.execute_reply":"2025-03-09T07:43:17.373836Z"}},"outputs":[],"execution_count":16},{"cell_type":"markdown","source":"**TODO**: The below `get_img_encodings` function is riddled with `FIXMEs`. Please replace each `FIXME` with the appropriate code to generate CLIP encodings from PIL images.\n\nClick the `...` for an answer.","metadata":{}},{"cell_type":"code","source":"def get_img_encodings(imgs):\n    processed_imgs = [FIXME(img) for img in imgs]\n    clip_imgs = torch.tensor(np.stack(FIXME)).to(device)\n    clip_img_encodings = FIXME.encode_image(clip_imgs)\n    return clip_img_encodings","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-09T07:43:17.375718Z","iopub.execute_input":"2025-03-09T07:43:17.376044Z","iopub.status.idle":"2025-03-09T07:43:17.381138Z","shell.execute_reply.started":"2025-03-09T07:43:17.376012Z","shell.execute_reply":"2025-03-09T07:43:17.379877Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"def get_img_encodings(imgs):\n    processed_imgs = [clip_preprocess(img) for img in imgs]\n    clip_imgs = torch.tensor(np.stack(processed_imgs)).to(device)\n    clip_img_encodings = clip_model.encode_image(clip_imgs)\n    return clip_img_encodings","metadata":{"jupyter":{"source_hidden":true},"trusted":true,"execution":{"iopub.status.busy":"2025-03-09T07:43:17.385585Z","iopub.execute_input":"2025-03-09T07:43:17.385806Z","iopub.status.idle":"2025-03-09T07:43:17.402149Z","shell.execute_reply.started":"2025-03-09T07:43:17.385786Z","shell.execute_reply":"2025-03-09T07:43:17.401158Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"clip_img_encodings = get_img_encodings(imgs)\nclip_img_encodings","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-09T07:43:17.404104Z","iopub.execute_input":"2025-03-09T07:43:17.404340Z","iopub.status.idle":"2025-03-09T07:43:17.449365Z","shell.execute_reply.started":"2025-03-09T07:43:17.404298Z","shell.execute_reply":"2025-03-09T07:43:17.448761Z"}},"outputs":[{"execution_count":19,"output_type":"execute_result","data":{"text/plain":"tensor([[-0.2717, -0.0157, -0.1792,  ...,  0.5811,  0.0866, -0.1448],\n        [ 0.1428, -0.0866, -0.2581,  ...,  0.5127,  0.2074, -0.0048],\n        [-0.1198,  0.5830, -0.0645,  ...,  0.3027,  0.3315,  0.1141]],\n       device='cuda:0', dtype=torch.float16, grad_fn=<MmBackward0>)"},"metadata":{}}],"execution_count":19},{"cell_type":"markdown","source":"**TODO**: Find text that describes the above images well and will result in a high similarity score. After calculating the similarity score, feel free to repeat this exercise and modify. We will be using this text list again later.\n\nClick the `...` for an example.","metadata":{}},{"cell_type":"code","source":"# text_list = [\n#     \"A daisy\",\n#     \"A sunflower\",\n#     \"A rose\"\n# ]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-09T07:43:17.449983Z","iopub.execute_input":"2025-03-09T07:43:17.450191Z","iopub.status.idle":"2025-03-09T07:43:17.453077Z","shell.execute_reply.started":"2025-03-09T07:43:17.450173Z","shell.execute_reply":"2025-03-09T07:43:17.452404Z"}},"outputs":[],"execution_count":20},{"cell_type":"markdown","source":"```python\ntext_list = [\n    \"A white daisy with a yellow center\",\n    \"An orange sunflower with a big brown center\",\n    \"A deep red rose flower\"\n]\n```","metadata":{}},{"cell_type":"code","source":"# text_tokens = clip.tokenize(text_list).to(device)\n# clip_text_encodings = clip_model.encode_text(text_tokens).float()\n# clip_text_encodings","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-09T07:43:17.453740Z","iopub.execute_input":"2025-03-09T07:43:17.453968Z","iopub.status.idle":"2025-03-09T07:43:17.468185Z","shell.execute_reply.started":"2025-03-09T07:43:17.453950Z","shell.execute_reply":"2025-03-09T07:43:17.467401Z"}},"outputs":[],"execution_count":21},{"cell_type":"markdown","source":"It would be nice to compare each combination of text and image. To do so, we can [repeat](https://pytorch.org/docs/stable/generated/torch.Tensor.repeat.html#torch.Tensor.repeat) each text encoding for each image encoding. Similarly, we can [repeat_interleave](https://pytorch.org/docs/stable/generated/torch.repeat_interleave.html) each image encoding for each text encoding.","metadata":{}},{"cell_type":"code","source":"clip_img_encodings /= clip_img_encodings.norm(dim=-1, keepdim=True)\nclip_text_encodings /= clip_text_encodings.norm(dim=-1, keepdim=True)\n\nn_imgs = len(imgs)\nn_text = len(text_list)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-09T07:43:17.468963Z","iopub.execute_input":"2025-03-09T07:43:17.469221Z","iopub.status.idle":"2025-03-09T07:43:17.484566Z","shell.execute_reply.started":"2025-03-09T07:43:17.469189Z","shell.execute_reply":"2025-03-09T07:43:17.483738Z"}},"outputs":[],"execution_count":22},{"cell_type":"code","source":"# repeated_clip_text_encodings = clip_text_encodings.repeat(n_imgs, 1)\n# repeated_clip_text_encodings","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-09T07:43:17.485379Z","iopub.execute_input":"2025-03-09T07:43:17.485659Z","iopub.status.idle":"2025-03-09T07:43:17.497983Z","shell.execute_reply.started":"2025-03-09T07:43:17.485640Z","shell.execute_reply":"2025-03-09T07:43:17.497324Z"}},"outputs":[],"execution_count":23},{"cell_type":"code","source":"repeated_clip_img_encoding = clip_img_encodings.repeat_interleave(n_text, dim=0)\nrepeated_clip_img_encoding","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-09T07:43:17.498859Z","iopub.execute_input":"2025-03-09T07:43:17.499155Z","iopub.status.idle":"2025-03-09T07:43:17.518732Z","shell.execute_reply.started":"2025-03-09T07:43:17.499136Z","shell.execute_reply":"2025-03-09T07:43:17.517876Z"}},"outputs":[{"execution_count":24,"output_type":"execute_result","data":{"text/plain":"tensor([[-0.0247, -0.0014, -0.0163,  ...,  0.0528,  0.0079, -0.0132],\n        [-0.0247, -0.0014, -0.0163,  ...,  0.0528,  0.0079, -0.0132],\n        [-0.0247, -0.0014, -0.0163,  ...,  0.0528,  0.0079, -0.0132],\n        ...,\n        [-0.0112,  0.0545, -0.0060,  ...,  0.0283,  0.0310,  0.0107],\n        [-0.0112,  0.0545, -0.0060,  ...,  0.0283,  0.0310,  0.0107],\n        [-0.0112,  0.0545, -0.0060,  ...,  0.0283,  0.0310,  0.0107]],\n       device='cuda:0', dtype=torch.float16, grad_fn=<ViewBackward0>)"},"metadata":{}}],"execution_count":24},{"cell_type":"code","source":"# similarity = (repeated_clip_text_encodings * repeated_clip_img_encoding).sum(-1)\n# similarity = torch.unflatten(similarity, 0, (n_text, n_imgs))\n# similarity","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-09T07:43:17.519450Z","iopub.execute_input":"2025-03-09T07:43:17.519674Z","iopub.status.idle":"2025-03-09T07:43:17.522936Z","shell.execute_reply.started":"2025-03-09T07:43:17.519655Z","shell.execute_reply":"2025-03-09T07:43:17.522174Z"}},"outputs":[],"execution_count":25},{"cell_type":"markdown","source":"Let's compare. Ideally, the diagonal from the top left to the bottom right should be a bright yellow corresponding to their high value. The rest of the values should be low and blue.","metadata":{}},{"cell_type":"code","source":"# fig = plt.figure(figsize=(10, 10))\n# gs = fig.add_gridspec(2, 3, wspace=.1, hspace=0)\n\n# for i, img in enumerate(imgs):\n#     ax = fig.add_subplot(gs[0, i])\n#     ax.axis(\"off\")\n#     plt.imshow(img)\n\n# ax = fig.add_subplot(gs[1, :])\n# plt.imshow(similarity.detach().cpu().numpy().T, vmin=0.1, vmax=0.3)\n\n# labels = [ '\\n'.join(wrap(text, 20)) for text in text_list ]\n# plt.yticks(range(n_text), labels, fontsize=10)\n# plt.xticks([])\n\n# for x in range(similarity.shape[1]):\n#     for y in range(similarity.shape[0]):\n#         plt.text(x, y, f\"{similarity[x, y]:.2f}\", ha=\"center\", va=\"center\", size=12)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-09T07:43:17.523764Z","iopub.execute_input":"2025-03-09T07:43:17.524033Z","iopub.status.idle":"2025-03-09T07:43:17.535917Z","shell.execute_reply.started":"2025-03-09T07:43:17.524007Z","shell.execute_reply":"2025-03-09T07:43:17.535236Z"}},"outputs":[],"execution_count":26},{"cell_type":"markdown","source":"## 5.2 A CLIP Dataset","metadata":{}},{"cell_type":"markdown","source":"In the previous notebook, we used the flower category as the label. This time, we're going to use CLIP encodings as our label.\n\nIf the goal of CLIP is to align text encodings with image encodings, do we need a text description for each of the images in our dataset? Hypothesis: we do not need text descriptions and only need the image CLIP encodings to create a text-to-image pipeline.\n\nTo test this out, let's add the CLIP encodings as the \"label\" to our dataset. Running CLIP on each batch of data augmented images would be more accurate, but it is also slower. We can speed things up by preprocessing and storing the encodings ahead of time.\n\nWe can use [glob](https://docs.python.org/3/library/glob.html) to list all of our image filepaths:","metadata":{}},{"cell_type":"code","source":"data_paths = glob.glob(DATA_DIR + '*/*.jpg', recursive=True)\ndata_paths[:5]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-09T07:43:17.536562Z","iopub.execute_input":"2025-03-09T07:43:17.536850Z","iopub.status.idle":"2025-03-09T07:43:17.646513Z","shell.execute_reply.started":"2025-03-09T07:43:17.536830Z","shell.execute_reply":"2025-03-09T07:43:17.645893Z"}},"outputs":[{"execution_count":27,"output_type":"execute_result","data":{"text/plain":"['/kaggle/input/hoahoa/flower_photos/dandelion/2625836599_03e192266f.jpg',\n '/kaggle/input/hoahoa/flower_photos/dandelion/16242239484_51286673af.jpg',\n '/kaggle/input/hoahoa/flower_photos/dandelion/5716633491_55e6f02645_n.jpg',\n '/kaggle/input/hoahoa/flower_photos/dandelion/13651218133_b6eb8e7ed2_m.jpg',\n '/kaggle/input/hoahoa/flower_photos/dandelion/8719032054_9a3ce4f0ff.jpg']"},"metadata":{}}],"execution_count":27},{"cell_type":"markdown","source":"The next code block runs the following loop for each filepath:\n* Open the image associated with the path and store it in `img`\n* Preprocess the image, find the CLIP encoding, and store it in `clip_img`\n* Convert the CLIP encoding from a tensor to a python list\n* Store the filepath and the CLIP encoding as a row in a csv file","metadata":{}},{"cell_type":"code","source":"csv_path = 'clip.csv'\n\nwith open(csv_path, 'w', newline='') as csvfile:\n    writer = csv.writer(csvfile, delimiter=',')\n    for idx, path in enumerate(data_paths):\n        img = Image.open(path)\n        clip_img = torch.tensor(np.stack([clip_preprocess(img)])).to(device)\n        label = clip_model.encode_image(clip_img)[0].tolist()\n        writer.writerow([path] + label)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-09T07:43:17.647167Z","iopub.execute_input":"2025-03-09T07:43:17.647418Z","iopub.status.idle":"2025-03-09T07:44:30.256266Z","shell.execute_reply.started":"2025-03-09T07:43:17.647390Z","shell.execute_reply":"2025-03-09T07:44:30.255372Z"}},"outputs":[],"execution_count":28},{"cell_type":"markdown","source":"It may take a few seconds to process the full dataset. When complete, open [clip.csv](clip.csv) to see the results.\n\nWe can use the same image transformations as we did with the other notebook:","metadata":{}},{"cell_type":"code","source":"IMG_SIZE = 32 # Due to stride and pooling, must be divisible by 2 multiple times\nIMG_CH = 3\nBATCH_SIZE = 128*4\nINPUT_SIZE = (IMG_CH, IMG_SIZE, IMG_SIZE)\n\npre_transforms = [\n    transforms.Resize(IMG_SIZE),\n    transforms.ToTensor(),  # Scales data into [0,1]\n    transforms.Lambda(lambda t: (t * 2) - 1)  # Scale between [-1, 1]\n]\npre_transforms = transforms.Compose(pre_transforms)\nrandom_transforms = [\n    transforms.RandomCrop(IMG_SIZE),\n    transforms.RandomHorizontalFlip(),\n]\nrandom_transforms = transforms.Compose(random_transforms)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-09T07:44:30.257163Z","iopub.execute_input":"2025-03-09T07:44:30.257462Z","iopub.status.idle":"2025-03-09T07:44:30.262258Z","shell.execute_reply.started":"2025-03-09T07:44:30.257432Z","shell.execute_reply":"2025-03-09T07:44:30.261491Z"}},"outputs":[],"execution_count":29},{"cell_type":"markdown","source":"Below is the code to initialize our new dataset. Since we've `preprocessed_clip`, we will preload it onto our GPU with the `__init__` function. We've kept the \"on the fly\" CLIP encoding as an example. It will produce slightly better results, but it is much slower.","metadata":{}},{"cell_type":"code","source":"class MyDataset(Dataset):\n    def __init__(self, csv_path, preprocessed_clip=True):\n        self.imgs = []\n        self.preprocessed_clip = preprocessed_clip\n        if preprocessed_clip:\n            self.labels = torch.empty(\n                len(data_paths), CLIP_FEATURES, dtype=torch.float, device=device\n            )\n        \n        with open(csv_path, newline='') as csvfile:\n            reader = csv.reader(csvfile, delimiter=',')\n            for idx, row in enumerate(reader):\n                img = Image.open(row[0])\n                self.imgs.append(pre_transforms(img).to(device))\n                if preprocessed_clip:\n                    label = [float(x) for x in row[1:]]\n                    self.labels[idx, :] = torch.FloatTensor(label).to(device)\n\n    def __getitem__(self, idx):\n        img = random_transforms(self.imgs[idx])\n        if self.preprocessed_clip:\n            label = self.labels[idx]\n        else:\n            batch_img = img[None, :, :, :]\n            encoded_imgs = clip_model.encode_image(clip_preprocess(batch_img))\n            label = encoded_imgs.to(device).float()[0]\n        return img, label\n\n    def __len__(self):\n        return len(self.imgs)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-09T07:44:30.263073Z","iopub.execute_input":"2025-03-09T07:44:30.263279Z","iopub.status.idle":"2025-03-09T07:44:30.280500Z","shell.execute_reply.started":"2025-03-09T07:44:30.263258Z","shell.execute_reply":"2025-03-09T07:44:30.279689Z"}},"outputs":[],"execution_count":30},{"cell_type":"code","source":"train_data = MyDataset(csv_path)\ndataloader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-09T07:44:30.281244Z","iopub.execute_input":"2025-03-09T07:44:30.281541Z","iopub.status.idle":"2025-03-09T07:44:44.270356Z","shell.execute_reply.started":"2025-03-09T07:44:30.281515Z","shell.execute_reply":"2025-03-09T07:44:44.269683Z"}},"outputs":[],"execution_count":31},{"cell_type":"markdown","source":"The U-Net model is the same architecture as last time, but with one small difference. Instead of using the number of classes as our `c_embed_dim`, we will use the number of `CLIP_FEATURES`. Last time, `c` might have stood for \"class\", but this time, it stands for \"context\". Thankfully, they both start with `c`, so we do not need to refactor the code to reflect this change in intention.","metadata":{}},{"cell_type":"code","source":"# Khởi tạo mô hình\n# model = SimpleModel()\nimport torch.nn as nn\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-09T07:44:44.271181Z","iopub.execute_input":"2025-03-09T07:44:44.271433Z","iopub.status.idle":"2025-03-09T07:44:44.276796Z","shell.execute_reply.started":"2025-03-09T07:44:44.271414Z","shell.execute_reply":"2025-03-09T07:44:44.275968Z"}},"outputs":[],"execution_count":32},{"cell_type":"code","source":"T = 400\nB_start = 0.0001\nB_end = 0.02\nB = torch.linspace(B_start, B_end, T).to(device)\n\nddpm = ddpm_utils.DDPM(B, device)\nmodel = UNet_utils.UNet(\n    T, IMG_CH, IMG_SIZE, down_chs=(256, 256, 512), t_embed_dim=8, c_embed_dim=CLIP_FEATURES\n)\nprint(\"Num params: \", sum(p.numel() for p in model.parameters()))\n# model_flowers = model.to(device)\n# Kiểm tra nếu có GPU\nif torch.cuda.is_available() and torch.cuda.device_count() > 1:\n    print(\"Sử dụng 2 GPU!\")\n    model = nn.DataParallel(model)  # Sử dụng cả 2 GPU\nmodel_flowers = model\n# Chuyển mô hình lên GPU\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-09T07:44:44.277570Z","iopub.execute_input":"2025-03-09T07:44:44.277808Z","iopub.status.idle":"2025-03-09T07:44:44.770744Z","shell.execute_reply.started":"2025-03-09T07:44:44.277779Z","shell.execute_reply":"2025-03-09T07:44:44.770057Z"}},"outputs":[{"name":"stdout","text":"Num params:  44900355\nSử dụng 2 GPU!\n","output_type":"stream"},{"execution_count":33,"output_type":"execute_result","data":{"text/plain":"DataParallel(\n  (module): UNet(\n    (down0): ResidualConvBlock(\n      (conv1): GELUConvBlock(\n        (model): Sequential(\n          (0): Conv2d(3, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n          (1): GroupNorm(8, 256, eps=1e-05, affine=True)\n          (2): GELU(approximate='none')\n        )\n      )\n      (conv2): GELUConvBlock(\n        (model): Sequential(\n          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n          (1): GroupNorm(8, 256, eps=1e-05, affine=True)\n          (2): GELU(approximate='none')\n        )\n      )\n    )\n    (down1): DownBlock(\n      (model): Sequential(\n        (0): GELUConvBlock(\n          (model): Sequential(\n            (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n            (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n            (2): GELU(approximate='none')\n          )\n        )\n        (1): GELUConvBlock(\n          (model): Sequential(\n            (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n            (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n            (2): GELU(approximate='none')\n          )\n        )\n        (2): RearrangePoolBlock(\n          (rearrange): Rearrange('b c (h p1) (w p2) -> b (c p1 p2) h w', p1=2, p2=2)\n          (conv): GELUConvBlock(\n            (model): Sequential(\n              (0): Conv2d(1024, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n              (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n              (2): GELU(approximate='none')\n            )\n          )\n        )\n      )\n    )\n    (down2): DownBlock(\n      (model): Sequential(\n        (0): GELUConvBlock(\n          (model): Sequential(\n            (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n            (1): GroupNorm(32, 512, eps=1e-05, affine=True)\n            (2): GELU(approximate='none')\n          )\n        )\n        (1): GELUConvBlock(\n          (model): Sequential(\n            (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n            (1): GroupNorm(32, 512, eps=1e-05, affine=True)\n            (2): GELU(approximate='none')\n          )\n        )\n        (2): RearrangePoolBlock(\n          (rearrange): Rearrange('b c (h p1) (w p2) -> b (c p1 p2) h w', p1=2, p2=2)\n          (conv): GELUConvBlock(\n            (model): Sequential(\n              (0): Conv2d(2048, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n              (1): GroupNorm(32, 512, eps=1e-05, affine=True)\n              (2): GELU(approximate='none')\n            )\n          )\n        )\n      )\n    )\n    (to_vec): Sequential(\n      (0): Flatten(start_dim=1, end_dim=-1)\n      (1): GELU(approximate='none')\n    )\n    (dense_emb): Sequential(\n      (0): Linear(in_features=32768, out_features=256, bias=True)\n      (1): ReLU()\n      (2): Linear(in_features=256, out_features=256, bias=True)\n      (3): ReLU()\n      (4): Linear(in_features=256, out_features=32768, bias=True)\n      (5): ReLU()\n    )\n    (sinusoidaltime): SinusoidalPositionEmbedBlock()\n    (t_emb1): EmbedBlock(\n      (model): Sequential(\n        (0): Linear(in_features=8, out_features=512, bias=True)\n        (1): GELU(approximate='none')\n        (2): Linear(in_features=512, out_features=512, bias=True)\n        (3): Unflatten(dim=1, unflattened_size=(512, 1, 1))\n      )\n    )\n    (t_emb2): EmbedBlock(\n      (model): Sequential(\n        (0): Linear(in_features=8, out_features=256, bias=True)\n        (1): GELU(approximate='none')\n        (2): Linear(in_features=256, out_features=256, bias=True)\n        (3): Unflatten(dim=1, unflattened_size=(256, 1, 1))\n      )\n    )\n    (c_embed1): EmbedBlock(\n      (model): Sequential(\n        (0): Linear(in_features=512, out_features=512, bias=True)\n        (1): GELU(approximate='none')\n        (2): Linear(in_features=512, out_features=512, bias=True)\n        (3): Unflatten(dim=1, unflattened_size=(512, 1, 1))\n      )\n    )\n    (c_embed2): EmbedBlock(\n      (model): Sequential(\n        (0): Linear(in_features=512, out_features=256, bias=True)\n        (1): GELU(approximate='none')\n        (2): Linear(in_features=256, out_features=256, bias=True)\n        (3): Unflatten(dim=1, unflattened_size=(256, 1, 1))\n      )\n    )\n    (up0): Sequential(\n      (0): Unflatten(dim=1, unflattened_size=(512, 8, 8))\n      (1): GELUConvBlock(\n        (model): Sequential(\n          (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n          (1): GroupNorm(32, 512, eps=1e-05, affine=True)\n          (2): GELU(approximate='none')\n        )\n      )\n    )\n    (up1): UpBlock(\n      (model): Sequential(\n        (0): ConvTranspose2d(1024, 256, kernel_size=(2, 2), stride=(2, 2))\n        (1): GELUConvBlock(\n          (model): Sequential(\n            (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n            (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n            (2): GELU(approximate='none')\n          )\n        )\n        (2): GELUConvBlock(\n          (model): Sequential(\n            (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n            (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n            (2): GELU(approximate='none')\n          )\n        )\n        (3): GELUConvBlock(\n          (model): Sequential(\n            (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n            (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n            (2): GELU(approximate='none')\n          )\n        )\n        (4): GELUConvBlock(\n          (model): Sequential(\n            (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n            (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n            (2): GELU(approximate='none')\n          )\n        )\n      )\n    )\n    (up2): UpBlock(\n      (model): Sequential(\n        (0): ConvTranspose2d(512, 256, kernel_size=(2, 2), stride=(2, 2))\n        (1): GELUConvBlock(\n          (model): Sequential(\n            (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n            (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n            (2): GELU(approximate='none')\n          )\n        )\n        (2): GELUConvBlock(\n          (model): Sequential(\n            (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n            (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n            (2): GELU(approximate='none')\n          )\n        )\n        (3): GELUConvBlock(\n          (model): Sequential(\n            (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n            (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n            (2): GELU(approximate='none')\n          )\n        )\n        (4): GELUConvBlock(\n          (model): Sequential(\n            (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n            (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n            (2): GELU(approximate='none')\n          )\n        )\n      )\n    )\n    (out): Sequential(\n      (0): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (1): GroupNorm(8, 256, eps=1e-05, affine=True)\n      (2): ReLU()\n      (3): Conv2d(256, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    )\n  )\n)"},"metadata":{}}],"execution_count":33},{"cell_type":"markdown","source":"The `get_context_mask` function will change a little bit. Since we're replacing our categorical input with a CLIP embedding, we no longer need to one-hot encode our label. We'll still randomly set values in our encoding to `0` to help the model learn without context.","metadata":{}},{"cell_type":"code","source":"def get_context_mask(c, drop_prob):\n    c_mask = torch.bernoulli(torch.ones_like(c).float() - drop_prob).to(device)\n    return c_mask","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-09T07:44:44.771542Z","iopub.execute_input":"2025-03-09T07:44:44.771865Z","iopub.status.idle":"2025-03-09T07:44:44.775617Z","shell.execute_reply.started":"2025-03-09T07:44:44.771834Z","shell.execute_reply":"2025-03-09T07:44:44.774787Z"}},"outputs":[],"execution_count":34},{"cell_type":"markdown","source":"Let's also recreate the `sample_flowers` function. This time, it will take our `text_list` as a parameter and convert it to a CLIP encoding. The `sample_w` function remains mostly the same and has been moved to the bottom of [ddpm_utils.py](utils/ddpm_utils.py).","metadata":{}},{"cell_type":"code","source":"def sample_flowers(text_list):\n    text_tokens = clip.tokenize(text_list).to(device)\n    c = clip_model.encode_text(text_tokens).float()\n    x_gen, x_gen_store = ddpm_utils.sample_w(model, ddpm, INPUT_SIZE, T, c, device)\n    return x_gen, x_gen_store","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-09T07:44:44.776450Z","iopub.execute_input":"2025-03-09T07:44:44.776724Z","iopub.status.idle":"2025-03-09T07:44:44.791758Z","shell.execute_reply.started":"2025-03-09T07:44:44.776696Z","shell.execute_reply":"2025-03-09T07:44:44.790967Z"}},"outputs":[],"execution_count":35},{"cell_type":"markdown","source":"Time to get training! After about `50` `epochs`, the model will start generating something recognizable, and at `100` it will hit its stride. What do you think? Do the generated images match your descriptions?","metadata":{}},{"cell_type":"code","source":"epochs= 2000\nc_drop_prob = 0.1\nlrate = 1e-4\nsave_dir = \"/kaggle/working/\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-09T07:44:44.792426Z","iopub.execute_input":"2025-03-09T07:44:44.792619Z","iopub.status.idle":"2025-03-09T07:44:44.807185Z","shell.execute_reply.started":"2025-03-09T07:44:44.792594Z","shell.execute_reply":"2025-03-09T07:44:44.806458Z"}},"outputs":[],"execution_count":36},{"cell_type":"code","source":"optimizer = torch.optim.Adam(model.parameters(), lr=lrate)\n\nmodel.train()\nfor epoch in range(epochs):\n    for step, batch in enumerate(dataloader):\n        optimizer.zero_grad()\n        t = torch.randint(0, T, (BATCH_SIZE,), device=device).float()\n        x, c = batch\n        c_mask = get_context_mask(c, c_drop_prob)\n        loss = ddpm.get_loss(model_flowers, x, t, c, c_mask)\n        loss.backward()\n        optimizer.step()\n\n    print(f\"Epoch {epoch} | Step {step:03d} | Loss: {loss.item()}\")\n    # if epoch % 5 == 0 or epoch == int(epochs - 1):\n    #     x_gen, x_gen_store = sample_flowers(text_list)\n    #     grid = make_grid(x_gen.cpu(), nrow=len(text_list))\n    #     save_image(grid, save_dir + f\"image_ep{epoch:02}.png\")\n    #     print(\"saved images in \" + save_dir + f\" for episode {epoch}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-09T07:44:44.808018Z","iopub.execute_input":"2025-03-09T07:44:44.808232Z"}},"outputs":[{"name":"stdout","text":"Epoch 0 | Step 006 | Loss: 0.6796157360076904\nEpoch 1 | Step 006 | Loss: 0.3263826072216034\nEpoch 2 | Step 006 | Loss: 0.22803233563899994\nEpoch 3 | Step 006 | Loss: 0.2010236233472824\nEpoch 4 | Step 006 | Loss: 0.1728401482105255\nEpoch 5 | Step 006 | Loss: 0.16888009011745453\nEpoch 6 | Step 006 | Loss: 0.1632636934518814\nEpoch 7 | Step 006 | Loss: 0.1544860452413559\nEpoch 8 | Step 006 | Loss: 0.1492462158203125\nEpoch 9 | Step 006 | Loss: 0.15550975501537323\nEpoch 10 | Step 006 | Loss: 0.14870654046535492\nEpoch 11 | Step 006 | Loss: 0.15300780534744263\nEpoch 12 | Step 006 | Loss: 0.1443943977355957\nEpoch 13 | Step 006 | Loss: 0.14224472641944885\nEpoch 14 | Step 006 | Loss: 0.13725729286670685\nEpoch 15 | Step 006 | Loss: 0.12582293152809143\nEpoch 16 | Step 006 | Loss: 0.12501448392868042\nEpoch 17 | Step 006 | Loss: 0.13008427619934082\nEpoch 18 | Step 006 | Loss: 0.1396411806344986\nEpoch 19 | Step 006 | Loss: 0.12098126858472824\nEpoch 20 | Step 006 | Loss: 0.12060441821813583\nEpoch 21 | Step 006 | Loss: 0.13082805275917053\nEpoch 22 | Step 006 | Loss: 0.11818068474531174\nEpoch 23 | Step 006 | Loss: 0.11643783748149872\nEpoch 24 | Step 006 | Loss: 0.1262872964143753\nEpoch 25 | Step 006 | Loss: 0.10545854270458221\nEpoch 26 | Step 006 | Loss: 0.12384790182113647\nEpoch 27 | Step 006 | Loss: 0.10661043971776962\nEpoch 28 | Step 006 | Loss: 0.10530807822942734\nEpoch 29 | Step 006 | Loss: 0.10997482389211655\nEpoch 30 | Step 006 | Loss: 0.11873573064804077\nEpoch 31 | Step 006 | Loss: 0.10704061388969421\nEpoch 32 | Step 006 | Loss: 0.09553590416908264\nEpoch 33 | Step 006 | Loss: 0.1013939380645752\nEpoch 34 | Step 006 | Loss: 0.11485953629016876\nEpoch 35 | Step 006 | Loss: 0.10321198403835297\nEpoch 36 | Step 006 | Loss: 0.10227134078741074\nEpoch 37 | Step 006 | Loss: 0.09938016533851624\nEpoch 38 | Step 006 | Loss: 0.09731099009513855\nEpoch 39 | Step 006 | Loss: 0.09619921445846558\nEpoch 40 | Step 006 | Loss: 0.086719810962677\nEpoch 41 | Step 006 | Loss: 0.10198133438825607\nEpoch 42 | Step 006 | Loss: 0.11588134616613388\nEpoch 43 | Step 006 | Loss: 0.10701583325862885\nEpoch 44 | Step 006 | Loss: 0.08377084136009216\nEpoch 45 | Step 006 | Loss: 0.093895822763443\nEpoch 46 | Step 006 | Loss: 0.09646672010421753\nEpoch 47 | Step 006 | Loss: 0.09345415234565735\nEpoch 48 | Step 006 | Loss: 0.09074702858924866\nEpoch 49 | Step 006 | Loss: 0.09178212285041809\nEpoch 50 | Step 006 | Loss: 0.09627018868923187\nEpoch 51 | Step 006 | Loss: 0.09289462864398956\nEpoch 52 | Step 006 | Loss: 0.08061335235834122\nEpoch 53 | Step 006 | Loss: 0.0856480598449707\nEpoch 54 | Step 006 | Loss: 0.08367669582366943\nEpoch 55 | Step 006 | Loss: 0.09043826162815094\nEpoch 56 | Step 006 | Loss: 0.09325950592756271\nEpoch 57 | Step 006 | Loss: 0.08475237339735031\nEpoch 58 | Step 006 | Loss: 0.0928938016295433\nEpoch 59 | Step 006 | Loss: 0.08076703548431396\nEpoch 60 | Step 006 | Loss: 0.09278681129217148\nEpoch 61 | Step 006 | Loss: 0.08494748920202255\nEpoch 62 | Step 006 | Loss: 0.09507952630519867\nEpoch 63 | Step 006 | Loss: 0.08877076953649521\nEpoch 64 | Step 006 | Loss: 0.08281493932008743\nEpoch 65 | Step 006 | Loss: 0.07529358565807343\nEpoch 66 | Step 006 | Loss: 0.07698005437850952\nEpoch 67 | Step 006 | Loss: 0.08397375047206879\nEpoch 68 | Step 006 | Loss: 0.08456794917583466\nEpoch 69 | Step 006 | Loss: 0.0944075807929039\nEpoch 70 | Step 006 | Loss: 0.08855545520782471\nEpoch 71 | Step 006 | Loss: 0.0824153870344162\nEpoch 72 | Step 006 | Loss: 0.07900907099246979\nEpoch 73 | Step 006 | Loss: 0.08728507161140442\nEpoch 74 | Step 006 | Loss: 0.07911038398742676\nEpoch 75 | Step 006 | Loss: 0.09053488820791245\nEpoch 76 | Step 006 | Loss: 0.08982949703931808\nEpoch 77 | Step 006 | Loss: 0.07231830060482025\nEpoch 78 | Step 006 | Loss: 0.07785935699939728\nEpoch 79 | Step 006 | Loss: 0.08290542662143707\nEpoch 80 | Step 006 | Loss: 0.08255387842655182\nEpoch 81 | Step 006 | Loss: 0.08595609664916992\nEpoch 82 | Step 006 | Loss: 0.07728426903486252\nEpoch 83 | Step 006 | Loss: 0.07600412517786026\nEpoch 84 | Step 006 | Loss: 0.08643575757741928\nEpoch 85 | Step 006 | Loss: 0.08096570521593094\nEpoch 86 | Step 006 | Loss: 0.07825513184070587\nEpoch 87 | Step 006 | Loss: 0.08767835795879364\nEpoch 88 | Step 006 | Loss: 0.08627140522003174\nEpoch 89 | Step 006 | Loss: 0.09358751028776169\nEpoch 90 | Step 006 | Loss: 0.07887861877679825\nEpoch 91 | Step 006 | Loss: 0.07882660627365112\nEpoch 92 | Step 006 | Loss: 0.082321397960186\nEpoch 93 | Step 006 | Loss: 0.08674894273281097\nEpoch 94 | Step 006 | Loss: 0.0745004266500473\nEpoch 95 | Step 006 | Loss: 0.07557865977287292\nEpoch 96 | Step 006 | Loss: 0.07347371429204941\nEpoch 97 | Step 006 | Loss: 0.07844902575016022\nEpoch 98 | Step 006 | Loss: 0.08309173583984375\nEpoch 99 | Step 006 | Loss: 0.06981577724218369\nEpoch 100 | Step 006 | Loss: 0.08096225559711456\nEpoch 101 | Step 006 | Loss: 0.0792793333530426\nEpoch 102 | Step 006 | Loss: 0.06434987485408783\nEpoch 103 | Step 006 | Loss: 0.07921361923217773\nEpoch 104 | Step 006 | Loss: 0.08219662308692932\nEpoch 105 | Step 006 | Loss: 0.08213278651237488\nEpoch 106 | Step 006 | Loss: 0.08782260119915009\nEpoch 107 | Step 006 | Loss: 0.08471472561359406\nEpoch 108 | Step 006 | Loss: 0.08332432806491852\nEpoch 109 | Step 006 | Loss: 0.07079954445362091\nEpoch 110 | Step 006 | Loss: 0.07570432126522064\nEpoch 111 | Step 006 | Loss: 0.07945702224969864\nEpoch 112 | Step 006 | Loss: 0.08765296638011932\nEpoch 113 | Step 006 | Loss: 0.08246177434921265\nEpoch 114 | Step 006 | Loss: 0.07411645352840424\nEpoch 115 | Step 006 | Loss: 0.07614605873823166\nEpoch 116 | Step 006 | Loss: 0.07644414156675339\nEpoch 117 | Step 006 | Loss: 0.06999369710683823\nEpoch 118 | Step 006 | Loss: 0.08343079686164856\nEpoch 119 | Step 006 | Loss: 0.08099457621574402\nEpoch 120 | Step 006 | Loss: 0.07524795830249786\nEpoch 121 | Step 006 | Loss: 0.08428961038589478\nEpoch 122 | Step 006 | Loss: 0.06880240887403488\nEpoch 123 | Step 006 | Loss: 0.07897640764713287\nEpoch 124 | Step 006 | Loss: 0.07647112011909485\nEpoch 125 | Step 006 | Loss: 0.07949818670749664\nEpoch 126 | Step 006 | Loss: 0.08079332858324051\nEpoch 127 | Step 006 | Loss: 0.08045843243598938\nEpoch 128 | Step 006 | Loss: 0.08250449597835541\nEpoch 129 | Step 006 | Loss: 0.08822991698980331\nEpoch 130 | Step 006 | Loss: 0.07876261323690414\nEpoch 131 | Step 006 | Loss: 0.07940206676721573\nEpoch 132 | Step 006 | Loss: 0.08062886446714401\nEpoch 133 | Step 006 | Loss: 0.07478415966033936\nEpoch 134 | Step 006 | Loss: 0.06775390356779099\nEpoch 135 | Step 006 | Loss: 0.0824255421757698\nEpoch 136 | Step 006 | Loss: 0.08012375980615616\nEpoch 137 | Step 006 | Loss: 0.08217577636241913\nEpoch 138 | Step 006 | Loss: 0.08558201789855957\nEpoch 139 | Step 006 | Loss: 0.07939787954092026\nEpoch 140 | Step 006 | Loss: 0.07172098010778427\nEpoch 141 | Step 006 | Loss: 0.07546448707580566\nEpoch 142 | Step 006 | Loss: 0.08343559503555298\nEpoch 143 | Step 006 | Loss: 0.07274186611175537\nEpoch 144 | Step 006 | Loss: 0.07559873908758163\nEpoch 145 | Step 006 | Loss: 0.07373212277889252\nEpoch 146 | Step 006 | Loss: 0.07105791568756104\nEpoch 147 | Step 006 | Loss: 0.07632484287023544\nEpoch 148 | Step 006 | Loss: 0.07525026798248291\nEpoch 149 | Step 006 | Loss: 0.06725353002548218\nEpoch 150 | Step 006 | Loss: 0.06909443438053131\nEpoch 151 | Step 006 | Loss: 0.07640225440263748\nEpoch 152 | Step 006 | Loss: 0.0756985992193222\nEpoch 153 | Step 006 | Loss: 0.07363910973072052\nEpoch 154 | Step 006 | Loss: 0.0759493038058281\nEpoch 155 | Step 006 | Loss: 0.07492086291313171\nEpoch 156 | Step 006 | Loss: 0.07273561507463455\nEpoch 157 | Step 006 | Loss: 0.0756760984659195\nEpoch 158 | Step 006 | Loss: 0.0751626193523407\nEpoch 159 | Step 006 | Loss: 0.06819000840187073\nEpoch 160 | Step 006 | Loss: 0.07839900255203247\nEpoch 161 | Step 006 | Loss: 0.0677306056022644\nEpoch 162 | Step 006 | Loss: 0.07679769396781921\nEpoch 163 | Step 006 | Loss: 0.07467048615217209\nEpoch 164 | Step 006 | Loss: 0.07971946895122528\nEpoch 165 | Step 006 | Loss: 0.0702294409275055\nEpoch 166 | Step 006 | Loss: 0.06946839392185211\nEpoch 167 | Step 006 | Loss: 0.07438730448484421\nEpoch 168 | Step 006 | Loss: 0.07156603038311005\nEpoch 169 | Step 006 | Loss: 0.07428176701068878\nEpoch 170 | Step 006 | Loss: 0.07383346557617188\nEpoch 171 | Step 006 | Loss: 0.07183099538087845\nEpoch 172 | Step 006 | Loss: 0.08125971257686615\nEpoch 173 | Step 006 | Loss: 0.0752752348780632\nEpoch 174 | Step 006 | Loss: 0.07457204163074493\nEpoch 175 | Step 006 | Loss: 0.07384126633405685\nEpoch 176 | Step 006 | Loss: 0.08062727749347687\nEpoch 177 | Step 006 | Loss: 0.07004953920841217\nEpoch 178 | Step 006 | Loss: 0.07553766667842865\nEpoch 179 | Step 006 | Loss: 0.06406867504119873\nEpoch 180 | Step 006 | Loss: 0.07716602087020874\nEpoch 181 | Step 006 | Loss: 0.07332358509302139\nEpoch 182 | Step 006 | Loss: 0.07033585011959076\nEpoch 183 | Step 006 | Loss: 0.07579421997070312\nEpoch 184 | Step 006 | Loss: 0.07428671419620514\nEpoch 185 | Step 006 | Loss: 0.0674125999212265\nEpoch 186 | Step 006 | Loss: 0.07233764231204987\nEpoch 187 | Step 006 | Loss: 0.06937497854232788\nEpoch 188 | Step 006 | Loss: 0.06895913928747177\nEpoch 189 | Step 006 | Loss: 0.06626570224761963\nEpoch 190 | Step 006 | Loss: 0.07031747698783875\nEpoch 191 | Step 006 | Loss: 0.06926628947257996\nEpoch 192 | Step 006 | Loss: 0.07847599685192108\nEpoch 193 | Step 006 | Loss: 0.0672590583562851\nEpoch 194 | Step 006 | Loss: 0.06774153560400009\nEpoch 195 | Step 006 | Loss: 0.07159028202295303\nEpoch 196 | Step 006 | Loss: 0.07755611091852188\nEpoch 197 | Step 006 | Loss: 0.07280874252319336\nEpoch 198 | Step 006 | Loss: 0.07716554403305054\nEpoch 199 | Step 006 | Loss: 0.07094971835613251\nEpoch 200 | Step 006 | Loss: 0.06726421415805817\nEpoch 201 | Step 006 | Loss: 0.06533381342887878\nEpoch 202 | Step 006 | Loss: 0.07092069834470749\nEpoch 203 | Step 006 | Loss: 0.06727980822324753\nEpoch 204 | Step 006 | Loss: 0.07624298334121704\nEpoch 205 | Step 006 | Loss: 0.07496887445449829\nEpoch 206 | Step 006 | Loss: 0.06741605699062347\nEpoch 207 | Step 006 | Loss: 0.06854797154664993\nEpoch 208 | Step 006 | Loss: 0.07372824102640152\nEpoch 209 | Step 006 | Loss: 0.07490327954292297\nEpoch 210 | Step 006 | Loss: 0.0732482522726059\nEpoch 211 | Step 006 | Loss: 0.07050716876983643\nEpoch 212 | Step 006 | Loss: 0.0732235237956047\nEpoch 213 | Step 006 | Loss: 0.07195443660020828\nEpoch 214 | Step 006 | Loss: 0.07097890973091125\nEpoch 215 | Step 006 | Loss: 0.06573700159788132\nEpoch 216 | Step 006 | Loss: 0.07263418287038803\nEpoch 217 | Step 006 | Loss: 0.07376763224601746\nEpoch 218 | Step 006 | Loss: 0.064697265625\nEpoch 219 | Step 006 | Loss: 0.07420569658279419\nEpoch 220 | Step 006 | Loss: 0.07090061902999878\nEpoch 221 | Step 006 | Loss: 0.07000130414962769\nEpoch 222 | Step 006 | Loss: 0.06394124031066895\nEpoch 223 | Step 006 | Loss: 0.07428799569606781\nEpoch 224 | Step 006 | Loss: 0.06616230309009552\nEpoch 225 | Step 006 | Loss: 0.07607384026050568\nEpoch 226 | Step 006 | Loss: 0.06106306239962578\nEpoch 227 | Step 006 | Loss: 0.06576787680387497\nEpoch 228 | Step 006 | Loss: 0.07140287756919861\nEpoch 229 | Step 006 | Loss: 0.0676259696483612\nEpoch 230 | Step 006 | Loss: 0.0733412504196167\nEpoch 231 | Step 006 | Loss: 0.07117006182670593\nEpoch 232 | Step 006 | Loss: 0.07579685747623444\nEpoch 233 | Step 006 | Loss: 0.06882402300834656\nEpoch 234 | Step 006 | Loss: 0.06451103836297989\nEpoch 235 | Step 006 | Loss: 0.0696578100323677\nEpoch 236 | Step 006 | Loss: 0.07444729655981064\nEpoch 237 | Step 006 | Loss: 0.08453375101089478\nEpoch 238 | Step 006 | Loss: 0.0652555450797081\nEpoch 239 | Step 006 | Loss: 0.06729547679424286\nEpoch 240 | Step 006 | Loss: 0.0792539119720459\nEpoch 241 | Step 006 | Loss: 0.07263006269931793\nEpoch 242 | Step 006 | Loss: 0.07978474348783493\nEpoch 243 | Step 006 | Loss: 0.06988215446472168\nEpoch 244 | Step 006 | Loss: 0.06702636182308197\nEpoch 245 | Step 006 | Loss: 0.07288231700658798\nEpoch 246 | Step 006 | Loss: 0.06732866913080215\nEpoch 247 | Step 006 | Loss: 0.06549042463302612\nEpoch 248 | Step 006 | Loss: 0.06726524233818054\nEpoch 249 | Step 006 | Loss: 0.06702924519777298\nEpoch 250 | Step 006 | Loss: 0.06830146908760071\nEpoch 251 | Step 006 | Loss: 0.07570500671863556\nEpoch 252 | Step 006 | Loss: 0.0686102956533432\nEpoch 253 | Step 006 | Loss: 0.07154769450426102\nEpoch 254 | Step 006 | Loss: 0.07518696784973145\nEpoch 255 | Step 006 | Loss: 0.06567128002643585\nEpoch 256 | Step 006 | Loss: 0.06891756504774094\nEpoch 257 | Step 006 | Loss: 0.06991428136825562\nEpoch 258 | Step 006 | Loss: 0.06182819604873657\nEpoch 259 | Step 006 | Loss: 0.07660388946533203\nEpoch 260 | Step 006 | Loss: 0.06844000518321991\nEpoch 261 | Step 006 | Loss: 0.07146590203046799\nEpoch 262 | Step 006 | Loss: 0.0646362453699112\nEpoch 263 | Step 006 | Loss: 0.06215213984251022\nEpoch 264 | Step 006 | Loss: 0.06432409584522247\nEpoch 265 | Step 006 | Loss: 0.06648736447095871\nEpoch 266 | Step 006 | Loss: 0.07059981673955917\nEpoch 267 | Step 006 | Loss: 0.06806953251361847\nEpoch 268 | Step 006 | Loss: 0.06083625555038452\nEpoch 269 | Step 006 | Loss: 0.06940363347530365\nEpoch 270 | Step 006 | Loss: 0.05649100989103317\nEpoch 271 | Step 006 | Loss: 0.06587638705968857\nEpoch 272 | Step 006 | Loss: 0.07474078983068466\nEpoch 273 | Step 006 | Loss: 0.07044464349746704\nEpoch 274 | Step 006 | Loss: 0.0718170627951622\nEpoch 275 | Step 006 | Loss: 0.06488765776157379\nEpoch 276 | Step 006 | Loss: 0.0727335661649704\nEpoch 277 | Step 006 | Loss: 0.05717235058546066\nEpoch 278 | Step 006 | Loss: 0.07493595778942108\nEpoch 279 | Step 006 | Loss: 0.07437171787023544\nEpoch 280 | Step 006 | Loss: 0.06289362162351608\nEpoch 281 | Step 006 | Loss: 0.06892716139554977\nEpoch 282 | Step 006 | Loss: 0.07167958468198776\nEpoch 283 | Step 006 | Loss: 0.06818725913763046\nEpoch 284 | Step 006 | Loss: 0.06021009385585785\nEpoch 285 | Step 006 | Loss: 0.0736764445900917\nEpoch 286 | Step 006 | Loss: 0.06636475771665573\nEpoch 287 | Step 006 | Loss: 0.06605027616024017\nEpoch 288 | Step 006 | Loss: 0.06872501969337463\nEpoch 289 | Step 006 | Loss: 0.06664842367172241\nEpoch 290 | Step 006 | Loss: 0.07115420699119568\nEpoch 291 | Step 006 | Loss: 0.06033885106444359\nEpoch 292 | Step 006 | Loss: 0.0673452690243721\nEpoch 293 | Step 006 | Loss: 0.07125987112522125\nEpoch 294 | Step 006 | Loss: 0.06357802450656891\nEpoch 295 | Step 006 | Loss: 0.06419022381305695\nEpoch 296 | Step 006 | Loss: 0.06852694600820541\nEpoch 297 | Step 006 | Loss: 0.06386372447013855\nEpoch 298 | Step 006 | Loss: 0.06322553753852844\nEpoch 299 | Step 006 | Loss: 0.07157501578330994\nEpoch 300 | Step 006 | Loss: 0.06437064707279205\nEpoch 301 | Step 006 | Loss: 0.07187946140766144\nEpoch 302 | Step 006 | Loss: 0.06865105777978897\nEpoch 303 | Step 006 | Loss: 0.07525207847356796\nEpoch 304 | Step 006 | Loss: 0.06434937566518784\nEpoch 305 | Step 006 | Loss: 0.06579769402742386\nEpoch 306 | Step 006 | Loss: 0.07224533706903458\nEpoch 307 | Step 006 | Loss: 0.06604154407978058\nEpoch 308 | Step 006 | Loss: 0.06211787462234497\nEpoch 309 | Step 006 | Loss: 0.058678608387708664\nEpoch 310 | Step 006 | Loss: 0.07088015973567963\nEpoch 311 | Step 006 | Loss: 0.06396084278821945\nEpoch 312 | Step 006 | Loss: 0.07049932330846786\nEpoch 313 | Step 006 | Loss: 0.06464415043592453\nEpoch 314 | Step 006 | Loss: 0.0608435682952404\nEpoch 315 | Step 006 | Loss: 0.07428812980651855\nEpoch 316 | Step 006 | Loss: 0.07441401481628418\nEpoch 317 | Step 006 | Loss: 0.07428435981273651\nEpoch 318 | Step 006 | Loss: 0.07041098177433014\nEpoch 319 | Step 006 | Loss: 0.07025302946567535\nEpoch 320 | Step 006 | Loss: 0.058823853731155396\nEpoch 321 | Step 006 | Loss: 0.06763049960136414\nEpoch 322 | Step 006 | Loss: 0.061560481786727905\nEpoch 323 | Step 006 | Loss: 0.06669088453054428\nEpoch 324 | Step 006 | Loss: 0.06144946813583374\nEpoch 325 | Step 006 | Loss: 0.07986535131931305\nEpoch 326 | Step 006 | Loss: 0.06541286408901215\nEpoch 327 | Step 006 | Loss: 0.06451873481273651\nEpoch 328 | Step 006 | Loss: 0.0701504796743393\nEpoch 329 | Step 006 | Loss: 0.0647498145699501\nEpoch 330 | Step 006 | Loss: 0.06944528967142105\nEpoch 331 | Step 006 | Loss: 0.06268323957920074\nEpoch 332 | Step 006 | Loss: 0.0704103484749794\nEpoch 333 | Step 006 | Loss: 0.0679706409573555\nEpoch 334 | Step 006 | Loss: 0.0653475672006607\nEpoch 335 | Step 006 | Loss: 0.05974232405424118\nEpoch 336 | Step 006 | Loss: 0.060147982090711594\nEpoch 337 | Step 006 | Loss: 0.06292121112346649\nEpoch 338 | Step 006 | Loss: 0.06527511775493622\nEpoch 339 | Step 006 | Loss: 0.07025161385536194\nEpoch 340 | Step 006 | Loss: 0.06012933701276779\nEpoch 341 | Step 006 | Loss: 0.06011151894927025\nEpoch 342 | Step 006 | Loss: 0.06384912133216858\nEpoch 343 | Step 006 | Loss: 0.06346414983272552\nEpoch 344 | Step 006 | Loss: 0.07411152869462967\nEpoch 345 | Step 006 | Loss: 0.06941429525613785\nEpoch 346 | Step 006 | Loss: 0.06628227978944778\nEpoch 347 | Step 006 | Loss: 0.05308873951435089\nEpoch 348 | Step 006 | Loss: 0.06249719113111496\nEpoch 349 | Step 006 | Loss: 0.0670364499092102\nEpoch 350 | Step 006 | Loss: 0.07509800791740417\nEpoch 351 | Step 006 | Loss: 0.061916910111904144\nEpoch 352 | Step 006 | Loss: 0.06125631183385849\nEpoch 353 | Step 006 | Loss: 0.06090771406888962\nEpoch 354 | Step 006 | Loss: 0.06336678564548492\nEpoch 355 | Step 006 | Loss: 0.0674719363451004\nEpoch 356 | Step 006 | Loss: 0.0665009468793869\nEpoch 357 | Step 006 | Loss: 0.07284577190876007\nEpoch 358 | Step 006 | Loss: 0.06499388813972473\nEpoch 359 | Step 006 | Loss: 0.06538969278335571\nEpoch 360 | Step 006 | Loss: 0.06238645315170288\nEpoch 361 | Step 006 | Loss: 0.06512074917554855\nEpoch 362 | Step 006 | Loss: 0.0632779598236084\nEpoch 363 | Step 006 | Loss: 0.05504317209124565\nEpoch 364 | Step 006 | Loss: 0.06809400022029877\nEpoch 365 | Step 006 | Loss: 0.060689013451337814\nEpoch 366 | Step 006 | Loss: 0.0650678500533104\nEpoch 367 | Step 006 | Loss: 0.07038053870201111\nEpoch 368 | Step 006 | Loss: 0.05954506993293762\nEpoch 369 | Step 006 | Loss: 0.0680738165974617\nEpoch 370 | Step 006 | Loss: 0.06268848478794098\nEpoch 371 | Step 006 | Loss: 0.05783756822347641\nEpoch 372 | Step 006 | Loss: 0.07099565118551254\nEpoch 373 | Step 006 | Loss: 0.06759919226169586\nEpoch 374 | Step 006 | Loss: 0.06522396951913834\nEpoch 375 | Step 006 | Loss: 0.06462065875530243\nEpoch 376 | Step 006 | Loss: 0.06434979289770126\nEpoch 377 | Step 006 | Loss: 0.06406545639038086\nEpoch 378 | Step 006 | Loss: 0.05232247710227966\nEpoch 379 | Step 006 | Loss: 0.0641835406422615\nEpoch 380 | Step 006 | Loss: 0.05135108157992363\nEpoch 381 | Step 006 | Loss: 0.06076764315366745\nEpoch 382 | Step 006 | Loss: 0.06307607144117355\nEpoch 383 | Step 006 | Loss: 0.06371761113405228\nEpoch 384 | Step 006 | Loss: 0.06040702760219574\nEpoch 385 | Step 006 | Loss: 0.06643053889274597\nEpoch 386 | Step 006 | Loss: 0.06487102061510086\nEpoch 387 | Step 006 | Loss: 0.06459109485149384\nEpoch 388 | Step 006 | Loss: 0.07004707306623459\nEpoch 389 | Step 006 | Loss: 0.06014829874038696\nEpoch 390 | Step 006 | Loss: 0.0680265873670578\nEpoch 391 | Step 006 | Loss: 0.05987463891506195\nEpoch 392 | Step 006 | Loss: 0.05667710304260254\nEpoch 393 | Step 006 | Loss: 0.06780797243118286\nEpoch 394 | Step 006 | Loss: 0.05958668515086174\nEpoch 395 | Step 006 | Loss: 0.0693044513463974\nEpoch 396 | Step 006 | Loss: 0.062445297837257385\nEpoch 397 | Step 006 | Loss: 0.06308171153068542\nEpoch 398 | Step 006 | Loss: 0.059384431689977646\nEpoch 399 | Step 006 | Loss: 0.07394535839557648\nEpoch 400 | Step 006 | Loss: 0.06451541185379028\nEpoch 401 | Step 006 | Loss: 0.062068402767181396\nEpoch 402 | Step 006 | Loss: 0.060577746480703354\nEpoch 403 | Step 006 | Loss: 0.06090584024786949\nEpoch 404 | Step 006 | Loss: 0.0616154782474041\nEpoch 405 | Step 006 | Loss: 0.06485961377620697\nEpoch 406 | Step 006 | Loss: 0.059418730437755585\nEpoch 407 | Step 006 | Loss: 0.07007312774658203\nEpoch 408 | Step 006 | Loss: 0.05774810537695885\nEpoch 409 | Step 006 | Loss: 0.06907331943511963\nEpoch 410 | Step 006 | Loss: 0.0643288642168045\nEpoch 411 | Step 006 | Loss: 0.06803830713033676\nEpoch 412 | Step 006 | Loss: 0.06533154845237732\nEpoch 413 | Step 006 | Loss: 0.06822293251752853\nEpoch 414 | Step 006 | Loss: 0.06338979303836823\nEpoch 415 | Step 006 | Loss: 0.07144558429718018\nEpoch 416 | Step 006 | Loss: 0.060236722230911255\nEpoch 417 | Step 006 | Loss: 0.06247212737798691\nEpoch 418 | Step 006 | Loss: 0.06187272071838379\nEpoch 419 | Step 006 | Loss: 0.06758188456296921\nEpoch 420 | Step 006 | Loss: 0.066661536693573\nEpoch 421 | Step 006 | Loss: 0.05341712757945061\nEpoch 422 | Step 006 | Loss: 0.06434856355190277\nEpoch 423 | Step 006 | Loss: 0.05914677679538727\nEpoch 424 | Step 006 | Loss: 0.06702709943056107\nEpoch 425 | Step 006 | Loss: 0.05630728602409363\nEpoch 426 | Step 006 | Loss: 0.06130608171224594\nEpoch 427 | Step 006 | Loss: 0.06308354437351227\nEpoch 428 | Step 006 | Loss: 0.06705564260482788\nEpoch 429 | Step 006 | Loss: 0.0729946494102478\nEpoch 430 | Step 006 | Loss: 0.06262203305959702\nEpoch 431 | Step 006 | Loss: 0.06344559043645859\nEpoch 432 | Step 006 | Loss: 0.06624551117420197\nEpoch 433 | Step 006 | Loss: 0.0720454528927803\nEpoch 434 | Step 006 | Loss: 0.06557820737361908\nEpoch 435 | Step 006 | Loss: 0.056024402379989624\nEpoch 436 | Step 006 | Loss: 0.060459356755018234\nEpoch 437 | Step 006 | Loss: 0.06885555386543274\nEpoch 438 | Step 006 | Loss: 0.06434133648872375\nEpoch 439 | Step 006 | Loss: 0.06400714814662933\nEpoch 440 | Step 006 | Loss: 0.06054425239562988\nEpoch 441 | Step 006 | Loss: 0.05804958939552307\nEpoch 442 | Step 006 | Loss: 0.05833868682384491\nEpoch 443 | Step 006 | Loss: 0.059098999947309494\nEpoch 444 | Step 006 | Loss: 0.056467682123184204\nEpoch 445 | Step 006 | Loss: 0.0639089047908783\nEpoch 446 | Step 006 | Loss: 0.06942244619131088\nEpoch 447 | Step 006 | Loss: 0.06065697222948074\nEpoch 448 | Step 006 | Loss: 0.07172814011573792\nEpoch 449 | Step 006 | Loss: 0.06987903267145157\nEpoch 450 | Step 006 | Loss: 0.06382382661104202\nEpoch 451 | Step 006 | Loss: 0.0554448738694191\nEpoch 452 | Step 006 | Loss: 0.06889091432094574\nEpoch 453 | Step 006 | Loss: 0.07275112718343735\nEpoch 454 | Step 006 | Loss: 0.0691356286406517\nEpoch 455 | Step 006 | Loss: 0.061228763312101364\nEpoch 456 | Step 006 | Loss: 0.06022806093096733\nEpoch 457 | Step 006 | Loss: 0.05997597426176071\nEpoch 458 | Step 006 | Loss: 0.05942533537745476\nEpoch 459 | Step 006 | Loss: 0.06455320119857788\nEpoch 460 | Step 006 | Loss: 0.05897475406527519\nEpoch 461 | Step 006 | Loss: 0.07088040560483932\nEpoch 462 | Step 006 | Loss: 0.06363068521022797\nEpoch 463 | Step 006 | Loss: 0.057923659682273865\nEpoch 464 | Step 006 | Loss: 0.0692472830414772\nEpoch 465 | Step 006 | Loss: 0.052830398082733154\nEpoch 466 | Step 006 | Loss: 0.0560358464717865\nEpoch 467 | Step 006 | Loss: 0.06129223853349686\nEpoch 468 | Step 006 | Loss: 0.06325164437294006\nEpoch 469 | Step 006 | Loss: 0.05724590644240379\nEpoch 470 | Step 006 | Loss: 0.06439533829689026\nEpoch 471 | Step 006 | Loss: 0.05952850729227066\nEpoch 472 | Step 006 | Loss: 0.0634254440665245\nEpoch 473 | Step 006 | Loss: 0.06439950317144394\nEpoch 474 | Step 006 | Loss: 0.0618121400475502\nEpoch 475 | Step 006 | Loss: 0.07065259665250778\nEpoch 476 | Step 006 | Loss: 0.06351421773433685\nEpoch 477 | Step 006 | Loss: 0.06796880066394806\nEpoch 478 | Step 006 | Loss: 0.06237369775772095\nEpoch 479 | Step 006 | Loss: 0.06274377554655075\nEpoch 480 | Step 006 | Loss: 0.06402263045310974\nEpoch 481 | Step 006 | Loss: 0.06792470067739487\nEpoch 482 | Step 006 | Loss: 0.06956187635660172\nEpoch 483 | Step 006 | Loss: 0.05831814929842949\nEpoch 484 | Step 006 | Loss: 0.0676211565732956\nEpoch 485 | Step 006 | Loss: 0.05082767829298973\nEpoch 486 | Step 006 | Loss: 0.060886919498443604\nEpoch 487 | Step 006 | Loss: 0.06892289221286774\nEpoch 488 | Step 006 | Loss: 0.059134792536497116\nEpoch 489 | Step 006 | Loss: 0.06416058540344238\nEpoch 490 | Step 006 | Loss: 0.06752662360668182\nEpoch 491 | Step 006 | Loss: 0.059247441589832306\nEpoch 492 | Step 006 | Loss: 0.06442863494157791\nEpoch 493 | Step 006 | Loss: 0.06172018498182297\nEpoch 494 | Step 006 | Loss: 0.05558057129383087\nEpoch 495 | Step 006 | Loss: 0.05652369558811188\nEpoch 496 | Step 006 | Loss: 0.06696149706840515\nEpoch 497 | Step 006 | Loss: 0.07493346929550171\nEpoch 498 | Step 006 | Loss: 0.05894918367266655\nEpoch 499 | Step 006 | Loss: 0.06303616613149643\nEpoch 500 | Step 006 | Loss: 0.053746651858091354\nEpoch 501 | Step 006 | Loss: 0.06451188027858734\nEpoch 502 | Step 006 | Loss: 0.05980091914534569\nEpoch 503 | Step 006 | Loss: 0.05755861848592758\nEpoch 504 | Step 006 | Loss: 0.05995180457830429\nEpoch 505 | Step 006 | Loss: 0.06316713988780975\nEpoch 506 | Step 006 | Loss: 0.06388942152261734\nEpoch 507 | Step 006 | Loss: 0.06264885514974594\nEpoch 508 | Step 006 | Loss: 0.06917005777359009\nEpoch 509 | Step 006 | Loss: 0.06665408611297607\nEpoch 510 | Step 006 | Loss: 0.06356055289506912\nEpoch 511 | Step 006 | Loss: 0.05447670817375183\nEpoch 512 | Step 006 | Loss: 0.0617598295211792\nEpoch 513 | Step 006 | Loss: 0.061213456094264984\nEpoch 514 | Step 006 | Loss: 0.05567002296447754\nEpoch 515 | Step 006 | Loss: 0.058009784668684006\nEpoch 516 | Step 006 | Loss: 0.054544221609830856\nEpoch 517 | Step 006 | Loss: 0.05457122251391411\nEpoch 518 | Step 006 | Loss: 0.06452460587024689\nEpoch 519 | Step 006 | Loss: 0.055401772260665894\nEpoch 520 | Step 006 | Loss: 0.0670081377029419\nEpoch 521 | Step 006 | Loss: 0.07149597257375717\nEpoch 522 | Step 006 | Loss: 0.06370064616203308\n","output_type":"stream"}],"execution_count":null},{"cell_type":"markdown","source":"Now that the model is trained, let's play with it! What happens when we give it a prompt of something not in the dataset? Or can you craft the perfect prompt to generate an image you can imagine?\n\nThe art of crafting a prompt to get the results you desire is called **prompt engineering**, and as shown here, is dependent on the kind of data the model is trained on.","metadata":{}},{"cell_type":"code","source":"# Change me\ntext_list = [\n    \"A daisy \",\n    \"A sunflower shine\",\n    \"A rose beautiful\"\n]\nmodel.eval()\nx_gen, x_gen_store = sample_flowers(text_list)\ngrid = make_grid(x_gen.cpu(), nrow=len(text_list))\nother_utils.show_tensor_image([grid])\nplt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Once you've found a set of images you enjoy, run the below cell to turn it into an animation. It will be saved to [05_images/flowers.gif](05_images/flowers.gif)","metadata":{}},{"cell_type":"code","source":"torch.save(model_flowers.state_dict(), \"/kaggle/working/model_flowers.pth\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-10T18:21:09.826426Z","iopub.execute_input":"2025-03-10T18:21:09.826830Z","iopub.status.idle":"2025-03-10T18:21:09.908088Z","shell.execute_reply.started":"2025-03-10T18:21:09.826798Z","shell.execute_reply":"2025-03-10T18:21:09.906878Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-8bc8974e2b7f>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_flowers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"/kaggle/working/model_flowers.pth\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mNameError\u001b[0m: name 'torch' is not defined"],"ename":"NameError","evalue":"name 'torch' is not defined","output_type":"error"}],"execution_count":1},{"cell_type":"code","source":"grids = [other_utils.to_image(make_grid(x_gen.cpu(), nrow=len(text_list))) for x_gen in x_gen_store]\nother_utils.save_animation(grids, \"/kaggle/working/flowers.gif\")","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}